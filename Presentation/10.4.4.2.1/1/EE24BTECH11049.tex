\documentclass{beamer}
\mode<presentation>
\usepackage{amsmath}
\setbeamertemplate{itemize items}[circle] 
\setbeamercolor{itemize item}{fg=black}
\usepackage{amssymb}
%\usepackage{advdate}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{url}
\usepackage{etex}
\def\UrlBreaks{\do\/\do-}
\usetheme{metropolis}
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,right]{author in head/foot}%
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}
\setbeamertemplate{navigation symbols}{}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
%\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\let\vec\mathbf

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}

\numberwithin{equation}{section}

\title{Numerical Solutions to Equations}
\author{Faraz Patnam\\ Dept. of Electrical Engg.,\\IIT Hyderabad.}

\date{\today} 
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section*{Outline}
\begin{frame}{Table of Contents}
    \tableofcontents
\end{frame}

\section{Problem}
\begin{frame}{Problem Statement}
    Find the values of k for which the quadratic equation $2x^2 + kx + 5 = 0$. So they've two equal roots.
\end{frame}

\section{Theoritical Solution}
\begin{frame}{Value Of k}
    \begin{itemize}
    \item If the roots are equal then the value of Discriminant is to $0$
        \begin{align}
            b^2 - 4ac &= 0\\
            k^2 - 4 \times 2 \times 5 &= 0\\
            k &= \pm 2\sqrt{10}
        \end{align}
    \item We get two equations since there are two values for k.
        \begin{align}
            2x^2 + 2\sqrt{10}x + 5 &= 0 \label{Quad1}\\
            2x^2 - 2\sqrt{10}x + 5 &= 0 \label{Quad2}
        \end{align}
\end{itemize}
\end{frame}
\section{Matrix Method}
\begin{frame}
  \frametitle{Characteristic Polynomial}
  For a Matrix $A$ of order $n$, the characteristic equation is given by,
\begin{align}
  det\brak{A - \lambda I} = a_n \lambda ^n + a_{n-1} \lambda ^{n-1} \cdots + a_0 = 0
 \end{align}
 \begin{itemize}
     \item We know that the roots of the characteristic polynomial are the eigenvalues of the matrix A. 
     \item We need to construct a matrix such we can find the eigen value using any iterative algorithm.
 \end{itemize}
\end{frame}

\begin{frame}{Companion Matrix}
    Matrix A is said to be the companion of a polynomial $f\brak{x}$ if
    \begin{align}
        det\brak{A - \lambda I} = 0 \implies f\brak{x} = 0
    \end{align}
    For,
    \begin{align}
      f\brak{x} = c_0 + c_1 x + \cdots + x^n
    \end{align}
    The companion matrix is,
    \begin{align}
      \myvec{
        0 & 0 & \cdots & 0 & -c_0\\
        1 & 0 & \cdots & 0 & -c_1\\
        0 & 1 & \cdots & 0 & -c_2\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & \cdots & 1 & -c_{n-1}
      }
\end{align}
\end{frame}

\begin{frame}{My Companion Matrix}
    \begin{itemize}
        \item For $x^2 + \sqrt{10}x + 2.5 = 0$\\
        Companion matrix
        \begin{align}
            C_1 = \myvec{0 & -2.5 \\ 1 & -\sqrt{10}}
        \end{align}
        \item For $x^2 - \sqrt{10}x + 2.5 = 0$\\
        Companion matrix
        \begin{align}
            C_2 = \myvec{0 & -2.5 \\ 1 & \sqrt{10}}
        \end{align}
    \end{itemize}
\end{frame}

\begin{frame}{Matrix Method}
    1. Construct the companion matrix of the obtained polynomials.\\
    2. Use power iteration algorithm to find the largest eigen value of the companion matrix, called as $\lambda_{\text{max}}$.\\
    3. This eigen value obtained is the root of the polynomial.\\
    4. Now divide the polynomial with $x - \lambda_{\text{max}}$ to remove the root.\\
    5. Repeat the above steps iteratively on the reduced polynomial until a degree-1 equation is obtained, which gives the final root directly. \\
\end{frame}
\subsection{Power Iteration}

\begin{frame}{Power Iteration}
    The power iteration algorithm computes the largest eigenvalue $\lambda_{\text{max}}$ of this matrix by iteratively applying the following steps:
	\begin{align}
		\mathbf{\Tilde{x}}_{n} &= C \mathbf{x}_{n-1}, \\
		\mathbf{x}_{n} &= \frac{\mathbf{\Tilde{x}}_{n}}{\|\mathbf{\Tilde{x}}_{n}\|},
	\end{align}
    The iteration stops when:
	\begin{align}
		|\lambda_{\text{max}}^{(n)} - \lambda_{\text{max}}^{(n-1)}| < \epsilon,
	\end{align}
	where $\epsilon$ is the desired precision, and $\lambda_{n} = \frac{\mathbf{x}_{n}^\top C \mathbf{x}_{n}}{\mathbf{x}_{n}^\top \mathbf{x}_{n}}$.  \brak{\text{Rayleigh Quotient}}
    \end{frame}

\begin{frame}{Power Iteration}
	Once $\lambda_{\text{max}}$ is found, synthetic division is performed to reduce the polynomial:
	\begin{align}
		P(x) &= P(x) \div (x - \lambda_{\text{max}}), \\
		P(x) &= c_{1,0} + c_{1,1}x + c_{1,2}x^{2} + \cdots + c_{1,(n-1)}x^{n-1}.
	\end{align}

    Now, the new companion matrix will be evaluated. \\ This process is repeated iteratively until the polynomial is reduced to a degree-1 equation:
    \begin{align}
		P_1(x) = c_{n-1,0} + c_{n-1,1}x.
	\end{align}
	Thus, the update equation would be:
	\begin{align}
		P_{n-1}(x) &= P_{n}(x) \div (x - \lambda_{n}),
	\end{align}
    where $\lambda_{n}$ can be found using the power iteration of the companion matrix of $P_{n}(x)$.  
\end{frame}

\begin{frame}{Why does it converge?}
    \begin{itemize}
        \item 1. Power iteration works when the matrix $A$ has a dominant eigen value $\lambda_{\text{max}}$
        \begin{align}
            \abs{\lambda_1} > \abs{\lambda_2} \geq \abs{\lambda_3} \geq \dots \geq \abs{\lambda_n}
        \end{align}
        \item 2. Corresponding eigen vector is $v_1$\\
        \textbf{Eigen basis:}
        \item 3. Assume $v_1, v_2, \dots, v_n$ be the eigen values of matrix $A$.
        \item 4. Then the initial vector $x_0$ can be expressed as linear combination of eigen values
        \begin{align}
            x_0 = c_1v_1 + c_2v_2 + \dots + c_nv_n
        \end{align}
    \end{itemize}
\end{frame}

\begin{frame}{Result}
Quad1:\\
Root 1: -1.582720\\
Root 2: -1.579558\\

Quad2:\\
Root 1: 1.582720\\
Root 2: 1.579558
\end{frame}
\begin{frame}{Why does it converge?}
    After $k$ iterations, vector $x_k$ becomes
    \begin{align}
        x_k = \frac{c_1\lambda_1^kv_1 + c_2\lambda_2^kv_2 + \dots + c_n\lambda_n^kv_n}{\norm{c_1\lambda_1^kv_1 + c_2\lambda_2^kv_2 + \dots + c_n\lambda_n^kv_n}}
    \end{align}
    since $\lambda_1$ is the dominant eigen value as k increases (more precisely $k \to \infty$) the term $c_1\lambda_1^kv_1$ dominates and all the other terms become negligible\\
    The normalization step $\Vec{x}_k = \frac{\Vec{\Tilde{x}}_k}{\norm{\Vec{\Tilde{x}}_k}}$ensures that the vector $x_k$ does not grow indefinitely and stabilizes the iteration.
\end{frame}

\begin{frame}{Rate of Convergence}
    The rate of convergence of power iteration depends on the ratio of the second-largest eigenvalue to the largest eigenvalue:
    \begin{align}
        r = \abs{\frac{\lambda_2}{\lambda_1}}
    \end{align}
    1. If $r$ is small, the algorithm converges quickly.\\
    2. If $r$ is close to 1, the algorithm converges slowly.
\end{frame}

\end{document}g
